---
weight: 40
title: Components
description: StreamingFast Firehose component documentation
---

# Components

### Firehose Component Family

#### Components by Name

The Firehose system is comprised of several key components including the Deepmind-enabled Blockchain Node, the [Mindreader](components.md#mindreader), the [Merger](components.md#merger), the [Relayer](components.md#relayer), and the [Firehose gRPC Server](components.md#firehose-grpc-server).&#x20;

#### Component Relationships

The Firehose components work together in symphony to provide blockchain data from configured and instrumented nodes to consumers through the gRPC Server.

Understanding the Firehose components individually is helpful for fully comprehending the overall system and should aid with the setup and operation of the application.

### Mindreader

#### Mindreader in Detail

The Mindreader component is responsible for extracting data from instrumented blockchain nodes.

Firehose's Mindreader component utilizes the StreamingFast [node-manager](https://github.com/streamingfast/node-manager) library to run a blockchain node instance as a sub-process.&#x20;

Once the process has been started The Extractor component reads the data being generated by the node.

#### Firehose Depends on Mindreader

It's important to note that the data consumed by the Extractor component will feed all other data sources in the Firehose system.

#### Bootable Nodes

Extractor nodes can be considered simple, fully bootable nodes. The Extractor nodes however do not have archiving capabilities or any additional features.

#### Data Flows from Mindreader

After Firehose has been instrumented on a node it will begin returning _substantial amounts of data_. The data will flow from the Extractor component into the rest of the components and Firehose system.

#### Mindreader Nomenclature

The Extractor component is sometimes referred to as the Mindreader. This nickname stems from deepmind instrumentation.

#### High Availability Data

Firehose is capable of providing highly available data and it was actually designed to do so.

#### Multiple Extractor Components

Using multiple Extractor components will ensure blocks are flowing efficiently through the designed target system.

#### Data Deduplication

Firehose was designed to deduplicate identical data produced by two Extractor component instances executing the same block.

#### Data Aggregation

Firehose also aggregates any forked blocks that would be seen by a single Extractor component and not seen by other Extractor components.

#### Component Cooperation

Adding Extractor components and dispersing each one geographically will result in the Extractor components actually racing to transfer blocks to the Relayer component. This cooperation between the Extractor and Relayer components significantly increases the performance of the overall Firehose system.&#x20;

### Merger

#### Merger in Detail

The Merger component is responsible for managing and shaping data flowing out of the Extractor component.&#x20;

#### Blocks Files

The Merger component produces what are referred to as "100-blocks files." The Merger component receives "one-block" files from Extractor components that are feeding the Merger.&#x20;

#### One-block Storage

The Merger component creates a one-block object store to produce the 100-blocks files.

#### Forks

All forks visited by an Extractor component will also be merged by the Merger component.

#### Merging Blocks

The merged 100-blocks files will be created each time the Merger component receives one hundred blocks of data from its associated Extractor component.&#x20;

#### Fork Data Awareness

The Merger component will produce the files when there are no additional forks that might occur. The StreamingFast bstream ForkableHandler provides support for fork data awareness in future merged blocks.

#### Merger Responsibilities

The Merger component will:&#x20;

* boot and try to start where it left off if a merged-seen.gob file is available.
* boot and start the next bundle in the last merged-block in storage if there is _**not**_ a merged-seen.gob file available.
* gather one-block-files and assemble them into a bundle. The bundle is written when the first blocks of the next bundle are older than 25 seconds or it contains at least one fully-linked segment of 100 blocks.
* keep a list of all seen (merged) blocks in the last `{merger-max-fixable-fork}.` A "seen" block is a block that has been merged by the current Merger component or an Extractor component.
* delete one-blocks that are older than `{merger-max-fixable-fork}` or have already been seen (merged), and recorded in the `merged-seen.gob` file.
* load missing blocks from storage if missing blocks or holes are encountered. The blocks in storage are loaded to fill the seen-blocks cache and the Merger component continues to the next bundle.
* add any previously unaccounted-for one-block files to the subsequent bundle. For instance bundle 500 might include block 429 if it were previously missed during the merging process. Also, note that any blocks older than `{merger-max-fixable-fork}` will be deleted.

#### High Availability Merger&#x20;

A single Merger component is required for Extractor nodes in a highly available system.&#x20;

Highly available systems usually connect to the Relayer component to receive real-time blocks. Merged blocked files are used when Relayer components can't provide the requested data or satisfy a range.

Restarts from other components can be sustained and time provided for Merger components to be down when Relayer components provide 200 to 300 blocks in RAM.

Merged blocks generally aren't read by other Firehose components in a running, live highly available system.

### Relayer

#### Relayer in Detail

The Relayer component is responsible for providing executed block data to other components in the Firehose system.

The Relayer component feeds from all available Extractor nodes to get a comprehensive view of all possible forks.&#x20;

The Relayer "fans out", or relays, block information to the other components in the Firehose system.

#### Relayer & gRPC

The Relayer component serves its block data through the streaming gRPC interface `BlockStream::Blocks`. This is the _same interface_ that the Extractor component exposes to the Relayer component. Read more about the `BlockStream::Blocks`interface in [its GitHub repository](https://github.com/streamingfast/proto/blob/develop/sf/bstream/v1/bstream.proto).

#### High Availability Relayer

A Relayer component in highly available systems will feed from all of the Extractor nodes to gain a complete view of all possible forks.

Multiple Extractor components will ensure blocks are flowing efficiently to the Relayer component and throughout the rest of the Firehose system.

### Firehose gRPC Server

#### gRPC Server in Detail

The Firehose gRPC Server component is responsible for providing the extracted, formed, and collated blockchain data handled by the other Firehose components.

#### Historical Data

Firehose will use merged blocks from data storage directly for historical requests.&#x20;

#### Live Data

Live blocks are received from the Relayer component.&#x20;

#### Relayer & Extractor Coordination

The Relayer component gets its data from one, or more, Extractor components.

#### Serving Data

The Firehose gRPC Server component provides the data to the end consumer of the Firehose system through remote method calls to the server.

#### High Availability gRPC

Firehose can be scaled horizontally to provide a highly available system.&#x20;

The network speed and data throughput between consumers and Firehose deployments will dictate the speed of data availability.&#x20;

In addition, the network speed and data throughput between Relayer components and Firehose gRPC Server components will impact the speed of data availability.

Firehose gRPC Server components have the ability to connect to a subset of Relayer components or all Relayers available.

When the Firehose gRPC Server component is connected to all available Relayer components the probability that all forks will be viewed increases. Inbound requests made by consumers will be fulfilled with in-memory fork data.

Block navigation can be delayed when forked data isn't completely communicated to the Firehose gRPC Server component.&#x20;

Understanding how data flows through the Firehose system is beneficial for harnessing its full power. Additional information is provided further explaining the data flow through Firehose.
