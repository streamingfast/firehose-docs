---
weight: 20
title: Design Principles
description: StreamingFast Firehose design principles
---

# Design Principles

The Firehose system was heavily inspired by large-scale data science machinery and other processes previously developed by the StreamingFast team.

#### StreamingFast Firehose system "North Star"

The Firehose system was designed with the following truths and assumptions taken into excruciatingly careful consideration.

* Flat files provide more efficiency than live running CPU and RAM-consuming and intensive processes.
* Fast iteration is preferred for data processes because data is inherently messy.
* Data agility is only achievable when data processes can be parallelized.
* Clear data contracts between tasks and processes including APIs, RPC query formats, and data model definitions, are critical
* Maximum precision is required for defining, referencing, and identifying concepts or data models. _Leave no stone unturned._
* The only guarantee in data science is that data processes change and evolve.
* Migrating data is annoying, careful consideration must be taken for:
  * file formats,
  * forward and backward compatibility,
  * versioning,
  * and performance.

### Extraction

StreamingFast strives to create the shortest path available from the deterministic execution of blocks and transactions down into a flat file. High-level goals surrounding the extraction process were identified and conceptualized including:

* The development of simple, robust, laser-focused processes.&#x20;
* Create core system components including the `Extractor`, `Merger`, `Relayer`, and `Firehose gRPC Server.`
* Avoid coupling extraction and indexing and any other services.
* Guarantee maximum node performance during data extraction for instrumented nodes, for all protocols.

### Data Completeness

The Firehose system achieves data completeness through the extraction of all available data from instrumented nodes.&#x20;

Revisiting instrumented nodes is avoided by the Firehose system due to the complete, rich, verifiable data collected during the extraction process.

During a transaction, the balance for an address could change from `100` to `200.` Firehose will save the storage key that was altered, and the previous and next values.&#x20;

Forward and backward atomic updates and integrity checks are made possible due to the fidelity of data being tracked by Firehose.&#x20;

In the example above, `200` should be the next changed value for the `previous_data` key. If a discrepancy is encountered it means there is an issue with the extraction mechanism and data quality will be negatively impacted.

Complete data means accounting for:&#x20;

* the relationships between a transaction,&#x20;
* the transaction's block’s schedule,&#x20;
* transaction execution,&#x20;
* transaction expiration,
* events produced by any transaction side effects,
* the transaction call tree, and each call’s state transition and effects.&#x20;

Determining detailed transaction relationship information is difficult; especially after the fact.

Firehose provides thorough and complete transaction data to avoid missed opportunities for potential data application development efforts.

Query requests for either transaction status or state are available for some JSON-RPC protocols. Both status and state however aren't available.

Data processes triggered by Ethereum log events can benefit from having knowledge of their source. The event could have been generated by the current contract, its parent (contract), or another known and trusted contact.

Accessing rich, complete data leads smart contract developers to emit additional events. Emitting additional events leads to increased gas fees.

\--- CONTINUE HERE ---

Enriched and complete transaction data is simply not easily or readily available.&#x20;

The availability of this rich data even has effects on contract design. Contract designers are required to think about how the stored data will be queried and consumed by their application.

Richer external data processes allow developers to simplify contracts and reduce on-chain operation costs.

### Modeling With Extreme Care

The data model we use to ingest protocol data was modeled with extreme care. We discovered peculiarities of several protocols the hard way.

Some subtle interpretations of bits of data produced by a blockchain (e.g.: the meaning of a reverted call within the call stack of an Ethereum transaction) are such that, if enough information is not surfaced from the source, it can be impossible to interpret the data downstream. It is only when the model definitions (protobuf schemas) are complete and leave no bit of data in a node that we know that we can serve all needs: that no other solution is needed.

Conceptually, the data extracted from a node should be so complete that one could write a program that takes that data, and rebuilds a full archive node out of it, and is able to boot it.

***

### Use Files and Streams of Pure Data

As opposed to requests/responses model, we've chosen to use flat-files and data streams, to alleviate the challenges of querying pools of (often load-balanced) nodes in a master-to-master replication configuration.

This avoids massive consistency issues, their retries, the incurred latency, and greatly simplifies the consuming code.

Additionally, by adopting the flat-file and data stream abstractions we adhere to the Unix philosophy of writing programs that do one thing, do it well, and work together with other programs by handling streams of data as input and output.

***

### State Transition Hiearchy

We use state transitions scoped to a call, indexed within a transaction, indexed within a block.

Most blockchains “round up” state changes for all transactions into a block to facilitate consensus. But the basic unit of execution remains a single smart contract execution (a single EVM call alone, where calling another contract means a second execution).

Precision in state is therefore lost for what happens mid-block, i.e. when the state of a contract changes in the middle of a transaction, in the middle of a block. If you want to know the balance at the _exact_ point because it's required for some calculations (when you’re processing a log event for instance), you’re out of luck, because the node will provide the response that is true at the _end_ of that block. It's thus impossible to know if there are other transactions after the one you are indexing that mutated the same state again. Querying a node will potentially throw you off, sometimes egregiously so.

Not all chains make consuming the actual state easy. For example, Solidity makes such an endeavor rather opaque, in the form of a `bytes32` => `bytes32` mapping, although there are ways to decode it. However, making that state usable creates tremendous opportunities for indexing.

Regarding versioning, compatibility and speed of file content, we found Google’s Protocol Buffers version 3 to meet these last requirements, while striving for simplicity (e.g. as attested by their removal of optional/required fields in version 3).
